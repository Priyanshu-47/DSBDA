# -*- coding: utf-8 -*-
"""Practical Exam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13EFV617Erjen4k4bY6CFI3oBIwscVHB1
"""

#Q1. Locate open Iris dataset from the URL csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
#2. Perform the following operation on dataset
#a) Display total no of rows and column
#b) Display type of each column
#c) Sort the data in descending order , by considering column sepal.length
#d) Slice the data: 11 to 20 rows, and only two columns, sepal.length and Species
#e) rename the column Species to Type 
#f)Describe the dataset after renaming
#g)Apply normalization to all columns except species column

import pandas as pd
import numpy as np

df=pd.read_csv("/content/Iris.csv")

df

#a) Display total no of rows and column

df.shape

df.count

#Display type of each column

df.dtypes

#Sort the data in descending order , by considering column sepal.length

df.sort_values(['SepalLengthCm'], ascending=[False])

#Slice the data: 11 to 20 rows, and only two columns, sepal.length and Species

specific_data=df[["Id","SepalLengthCm","Species"]]

specific_data

sliced_data=specific_data[11:20]
print(sliced_data)

#rename the column Species to Type

newcols={
"Species" : "Type"}
 
df.rename(columns=newcols,inplace=True)

df

#Describe the dataset after renaming

df.describe(include='all')

#Apply normalisation

cols_to_norm = ['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']
df[cols_to_norm] = df[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))

cols_to_norm

#Q2.Consider the given dataset StudentsPerformanceTest1
#. Check that is there any missing values in dataframe as a whole
# is there any missing values across each column
# count of missing values across each column
# count row wise missing value
# count of missing values of a gender column.
# groupby count of missing values of a column , consider column gender and score
# replace the missing value of score column with average value of the column

import pandas as pd
import numpy as np

df1=pd.read_csv("/content/student-performance(MIS-VAL).csv")

df1

# Check that is there any missing values in dataframe as a whole

df1.isnull()

# is there any missing values across each column
# count of missing values across each column

df1.isnull().sum()

# count row wise missing value

df1.isnull().sum(axis = 1)

#count of missing values of a gender column.

df1['Gender'].isna().sum()

# groupby count of missing values of a column , consider column gender and score

df1.groupby(['Gender','Placement Score']).size()

df1.groupby(['Gender','Placement Score']).size().isnull().sum()

#replace the missing value of score column with average value of the column

df1['Math Score']=df1['Math Score'].fillna(df1['Math Score'].mean())

df1['Math Score']

#Q.3
#. Locate open Iris dataset from the URL csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' 
#2. Perform the label encoding , by considering Species as target variable.

import pandas as pd
import numpy as np

df=pd.read_csv("/content/Iris.csv")

df

#Perform the label encoding , by considering Species as target variable.

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder()

df['Species']= label_encoder.fit_transform(df['Species'])

df['Species']

#Q.4 . Locate open Iris dataset from the URL csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' 
#2. Perform the One Hot encoding , by considering Species as target variable.

df

# Perform the One Hot encoding , by considering Species as target variable.

dummies = pd.get_dummies(df.Species)

dummies

merged_data = pd.concat([df,dummies], axis=1)

merged_data

#Q.5 1. Locate open Iris dataset from the URL csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' 
#Perform the Dummy Variable Encoding , by considering Species as target variable.

df=pd.read_csv("/content/Iris.csv")

df

#Perform the Dummy Variable Encoding , by considering Species as target variable.

#Q.6  Creation of Dataset " StudentsPerformance" using Microsoft Excel.
#The features of dataset are: Math_Score, Reading_Score, Writing_Score, Placement_Score, Club_Join_Date 
#Range of Values: Math_Score [60-80], Reading_Score[75-,95], ,Writing_Score [60,80], Placement_Score[75-100], Club_Join_Date [2018-2021].
#Response VAriable : Placement Score
#In 20% data, fill the impurities. 

#(To be done in ms-excel)

#Q.7 Load the Academic Performance dataset in data frame object.
#Check null values in the dataset. 
#Check missing values in dataset and replace the null values with standard null value NaN
#Replace the missing value of Math Score with Mean Value
#Replace the missing value of Reading Score with standard deviation
#Replace the missing value of place with common value "Nashik"

import pandas as pd
import numpy as np

df=pd.read_csv("/content/student-performance(cat).csv")

df

#Check null values in the dataset

df.isnull().sum()

#Check missing values in dataset and replace the null values with standard null value NaN

df.fillna(value='NaN')

#Replace the missing value of Math Score with Mean Value

mean_value=df['Math Score'].mean()

df['Math Score'].fillna(value=mean_value, inplace=True)

df

#Replace the missing value of Reading Score with standard deviation

df['Reading Score'] = df['Reading Score'].fillna(df['Reading Score'].std())

df

#Replace the missing value of place with common value "Nashik"

df['Region'] = df['Region'].fillna(df['Region'].mode()[0])

df

#Q.8 1. Load the Academic Performance dataset in data frame object. 
# Check null values in the dataset.
#Count the number of null values in complete data set (Hint: eplace the null values with standard null value NaN)
#Dropping rows with at least 1 null value
#Dropping rows if all values in that row are missing
#Dropping columns with at least 1 null value.
#Dropping Rows with at least 1 null value in CSV file

import pandas as pd
import numpy as np

df=pd.read_csv("/content/student-performance(MIS-VAL).csv")

df

#Check null values in the dataset.
#Count the number of null values in complete data set (Hint: eplace the null values with standard null value NaN)

df.isnull().sum().sum()

#Count the number of null values in complete data set (Hint: eplace the null values with standard null value NaN)

df.fillna(value='NaN')

#Dropping rows with at least 1 null value

df.dropna()

#Dropping rows if all values in that row are missing

df.dropna(how = 'all')

df.dropna(axis = 1)

#Q.9 . Load the demo dataset in dataframe object df
# Detect the outlier using BoxPlot.
#Handle the outlier using Quantile based flooring and capping (Hint: the outlier is capped at a certain value above the 90th percentile value
#or floored at a factor below the 10th percentile value)

import pandas as pd
import numpy as np

df=pd.read_csv("/content/student-performance(outlier).csv")

df

# Detect the outlier using BoxPlot.
import matplotlib.pyplot as plt

plt.figure(figsize = (10, 7)) 

df.boxplot()

#Handle the outlier using Quantile based flooring and capping (Hint: the outlier is capped at a certain value above the 90th percentile value
#or floored at a factor below the 10th percentile value)

df=pd.read_csv("/content/student-performance(outlier).csv")

ninetieth_percentile = np.percentile(df['Math Score'], 90)

b = np.where(df['Math Score']>ninetieth_percentile,
ninetieth_percentile, df['Math Score'])
print("New array:",b)

df.insert(1,"m score",b,True)
df

#Q.10 1. Load the demo dataset in dataframe object df
#Detect the outlier using ScatterPlot
#Handle the outlier using Quantile based flooring and capping (Hint: the outlier is capped at a certain value above the 90th percentile value
#or floored at a factor below the 10th percentile value)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df=pd.read_csv("/content/student-performance(outlier).csv")

#Detect the outlier using ScatterPlot

fig, ax = plt.subplots(figsize = (18,10))
ax.scatter(df['Placement Score'], df['Placement offer count'])

#Handle the outlier using Quantile based flooring and capping (Hint: the outlier is capped at a certain value above the 90th percentile value
#or floored at a factor below the 10th percentile value)

#(Not Understood)

#Same as above

#Q.11 . Load the demo dataset in dataframe object df
#Detect the outlier using Z-score
#replace the outliers with the median value.

#(Not Understood)

import numpy as np
import pandas as pd
from scipy import stats
df=pd.read_csv("/content/student-performance(outlier).csv")
z=np.abs(stats.zscore(df['Math Score']))
print(z)
threshold=0.18
disp_outlier=np.where(z<threshold)
disp_outlier
#df.replace(to_replace=disp_outlier,value=df['math score'].median())

md=df['Math Score'].median()
md

df.replace(to_replace=disp_outlier,value=md)

#Q.12 Load the demo dataset in dataframe object df
#Detect the outlier using Inter Quantile Range(IQR)
#remove the outliers from the dataset

import numpy as np
import pandas as pd
df=pd.read_csv("/content/student-performance(outlier).csv")
sorted_data=sorted(df['Reading Score'])
sorted_data

q1=np.percentile(sorted_data,25)
q3=np.percentile(sorted_data,75)
print(q1,q3)
IQR=q3-q1

lower_bound=q1-(1.5*IQR)
uppr_bound=q3+(1.5*IQR)
print(lower_bound,uppr_bound)

r_outlier=[]
for i in sorted_data:
  if i<lower_bound or i>uppr_bound:
    r_outlier.append(i)
print(r_outlier)

newdf=df['Reading Score']
for i in r_outlier:
    newdf.drop(i,inplace=True)

newdf

#Q.13 1. Load the MallCustomer dataset in dataframe object df
#Display summary statistics (mean, median, minimum, maximum, standard deviation) for a dataset for each column
#Display Measures of Dispersion ( Mean Absolute Deviation, Variance, Standard Deviation, Range, Quartiles, Skewness)
#if your categorical variable is age groupsa nd quantitative variable is income, then provide summary statistics (minimum and maximum) of income 
#grouped by the age groups.

import pandas as pd
import numpy as np

df=pd.read_csv("/content/Mall_Customers.csv")

df

#Display summary statistics (mean, median, minimum, maximum, standard deviation) for a dataset for each column

print(df['Genre'].describe())

print(df['Age'].describe())

print('Annual Income (k$)')

print(df['Annual Income (k$)'].describe())

print('Spending Score (1-100)')

print(df['Spending Score (1-100)'].describe())

#Display Measures of Dispersion ( Mean Absolute Deviation, Variance, Standard Deviation, Range, Quartiles, Skewness)

#Mean Absolute Deviation

mean=df.mean()

mean

median=df.median()

median

mode=df.mode()

mode

#Standard Deviation
std=df.std()

std

#Variance
var=df.var()

var

#Skewness
skew=3*(mean-median)/std
print(skew)

#Quartiles
q1=np.percentile(df,25)
q3=np.percentile(df,75)
print("IQR =",q3-q1)

#Range
min=df.min()
max=df.max()
print("Range=",max-min)

grp=np.percentile(df,25)
grp

df

#Q.14Create a Linear Regression Model using Python to predict home prices using Boston Housing Dataset. The objective is to predict the value of prices of the 
#house using the given features.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_boston
boston=load_boston()
boston

data=pd.DataFrame(boston.data)
data.columns=boston.feature_names
data.head()

data['price']=boston.target
data['price']

data['PRICE']

x = data.drop(['price'], axis = 1)
y = data['price']

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=0)

import sklearn
from sklearn.linear_model import LinearRegression
lm=LinearRegression()
model=lm.fit(xtrain,ytrain)
ytrain_predict=lm.predict(xtrain)
ytest_predict=lm.predict(xtest)
print(ytrain_predict," ",ytest_predict)

#Q.15 1. Implement logistic regression using Python to perform classification on Social_Network_Ads.csv dataset.
#Compute Confusion matrix to find TP, FP, TN, FN, Accuracy, Error rate, Precision, Recall on the given dataset..

df=pd.read_csv("/content/Social_Network_Ads.csv")

df

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score

df=pd.read_csv("/content/Social_Network_Ads.csv")

df

from sklearn.linear_model import LogisticRegression
classifier=LogisticRegression(random_state=0)
classifier.fit(x_train,y_train)
y_pred=classifier.predict(x_test)

y_pred

from sklearn.metrics import confusion_matrix
cm=confusion_matrix(y_test,y_pred)
print(cm)

from sklearn.model_selection import cross_val_score
accuracies=cross_val_score(estimator=classifier,X= x_train,y= y_train,cv=4)
print(format(accuracies.mean()))

tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
print('Confusion matrix\n\n', cm)
print('\nTrue Positives(TP) = ', cm[0,0])
print('\nTrue Negatives(TN) = ', cm[1,1])
print('\nFalse Positives(FP) = ', cm[0,1])
print('\nFalse Negatives(FN) = ', cm[1,0])

#Q.16 1. Implement Simple NaÃ¯ve Bayes classification algorithm using Python/R on iris.csv dataset.
#2. Compute Confusion matrix to find TP, FP, TN, FN, Accuracy, Error rate, Precision, Recall on the given dataset.

import pandas as pd
import numpy as np

df=pd.read_csv("/content/Iris.csv")

from sklearn import datasets
from sklearn import metrics
from sklearn import preprocessing
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split as tts

df=datasets.load_iris()
data = pd.DataFrame({"sl":df.data[:,0], "sw":df.data[:,1], "pl":df.data[:,2], "pw":df.data[:,3], 'Species': df.target})

from sklearn.model_selection import train_test_split
X=data[['sl','sw','pl','pw']]
y=data["Species"]
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0)
X_train
X_test
y_train
y_test

from sklearn.metrics import make_scorer, accuracy_score,precision_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score ,precision_score,recall_score,f1_score
gaussian = GaussianNB()
gaussian.fit(X_train, y_train)
Y_pred = gaussian.predict(X_test) 
accuracy_nb=round(accuracy_score(y_test,Y_pred)* 100, 2)
acc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)

cm = confusion_matrix(y_test, Y_pred)
accuracy = accuracy_score(y_test,Y_pred)
precision =precision_score(y_test, Y_pred,average='micro')
recall =  recall_score(y_test, Y_pred,average='micro')
f1 = f1_score(y_test,Y_pred,average='micro')

print('Confusion matrix for Naive Bayes\n',cm)
print('accuracy_Naive Bayes: %.3f' %accuracy)
print('precision_Naive Bayes: %.3f' %precision)
print('recall_Naive Bayes: %.3f' %recall)
print('f1-score_Naive Bayes : %.3f' %f1)
MNB = MultinomialNB(alpha=0.6)
MNB.fit(X_train, y_train)
Y_pred = MNB.predict(X_test) 
accuracy_nb=round(accuracy_score(y_test,Y_pred)* 100, 2)
acc_MNB = round(MNB.score(X_train, y_train) * 100, 2)

#Q.17 Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words removal, Stemming and 
#Lemmatization.
#2. Create representation of document by calculating Term Frequency and Inverse Document Frequency.

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

#Tokenisation

text= "Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization."

from nltk.tokenize import sent_tokenize
tokenized_text=sent_tokenize(text)
tokenized_text

from nltk.tokenize import word_tokenize
tokenized_word= word_tokenize(text)
tokenized_word

# Stop words removal

from nltk.corpus import stopwords
stop_words=set(stopwords.words("english"))
print(stop_words)

text= "How to remove stop words with NLTK library in Python?"
import re
text= re.sub('[^a-zA-Z]', ' ',text)
text
tokens = word_tokenize(text.lower())
tokens


filtered_text=[]
for w in tokens:
  if w not in stop_words:
    filtered_text.append(w)
print("Tokenized Sentence:",tokens)
print("Filterd Sentence:",filtered_text)

#Stemming
from nltk.stem import PorterStemmer
e_words= ["wait", "waiting", "waited", "waits"]
ps =PorterStemmer()
for w in e_words:
  rootWord=ps.stem(w)
print(rootWord)

#Lemmatisation
from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()
text = "studies studying cries cry"
tokenization = nltk.word_tokenize(text)
for w in tokenization:
  print("Lemma for {} is {}".format(w,
wordnet_lemmatizer.lemmatize(w)))

#POS Tagging
import nltk
from nltk.tokenize import word_tokenize
data="The pink sweater fit her perfectly"
words=word_tokenize(data)
for word in words:
  print(nltk.pos_tag([word]))

#Create representation of document by calculating Term Frequency and Inverse Document Frequency.

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

documentA = 'Jupiter is the largest Planet'
documentB = 'Mars is the fourth planet from the Sun'

bagOfWordsA = documentA.split(' ')
bagOfWordsB = documentB.split(' ')
 
 
uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))

numOfWordsA = dict.fromkeys(uniqueWords, 0)
for word in bagOfWordsA:
    numOfWordsA[word] += 1
    numOfWordsB = dict.fromkeys(uniqueWords, 0)
for word in bagOfWordsB:
   numOfWordsB[word] += 1
 
 
numOfWordsA
 
numOfWordsB

def computeTF(wordDict, bagOfWords):
  TfDict = {}
  bagOfWordsCount = len(bagOfWords)
  for word, count in wordDict.items():
    TfDict[word] = count / float(bagOfWordsCount)
  return TfDict
TfA = computeTF(numOfWordsA, bagOfWordsA)
TfB = computeTF(numOfWordsB, bagOfWordsB)

def computeIDF(documents):
   import math
   N = len(documents)
   idfDict = dict.fromkeys(documents[0].keys(), 0)
   for document in documents:
    for word, val in document.items():
      if val > 0:
        idfDict[word] += 1
   for word, val in idfDict.items():
    idfDict[word] = math.log(N / float(val))
   return idfDict
idfs = computeIDF([numOfWordsA, numOfWordsB])
idfs

def computeTFIDF(tfBagOfWords, idfs):
    tfidf = {}
    for word, val in tfBagOfWords.items():
        tfidf[word] = val * idfs[word]
    return tfidf
 
 
tfidfA = computeTFIDF(TfA, idfs)
tfidfB = computeTFIDF(TfB, idfs)
 
 
df = pd.DataFrame([tfidfA, tfidfB])
df

#Q.18 1. Use the inbuilt dataset 'titanic'. Use the Seaborn library to see if we can find any patterns in the data.
#2. Write a code to check how the price of the ticket (column name: 'fare') for each passenger is distributed by plotting a histogram.

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import math
import numpy as np
import seaborn as sns  #library for statistical plotting
df = sns.load_dataset('titanic')

df

sns.distplot(df['fare'])

df["age"].mean()

df[["age", "fare"]].median()
df[["age", "fare"]].describe()

df.agg(
    {
        "age": ["min", "max", "median", "skew"],
        "fare": ["min", "max", "median", "mean"],
    }
)

df[["sex", "age"]].groupby("sex").mean()

df.groupby("sex").mean()

df.groupby("sex")["age"].mean()

df.groupby(["sex", "pclass"])["fare"].mean()

df["pclass"].value_counts()

sns.distplot(df['fare'], kde=False)

sns.distplot(df['fare'], kde=False, bins=10)

#Q.19 1. Use the inbuilt dataset 'titanic' as used in the above problem. Plot a box plot for distribution of age with respect to each gender along with the 
#information about whether they survived or not. (Column names : 'sex' and 'age')
#2. Write observations on the inference from the above statistics.

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import math
import numpy as np
import seaborn as sns  #library for statistical plotting
df = sns.load_dataset('titanic')

df
df.shape
df.head()
df.info()
df.describe()
print('no of passengeres:'+str(len(df.index)))

sns.countplot(x='survived',hue='sex',data=df)

sns.countplot(x='survived',hue='pclass',data=df)

sns.countplot(x='survived',hue='age',data=df)

df.isnull().sum()
df.drop('deck',axis=1,inplace=True)
df.head()

sns.boxplot(x='sex', y='age', data=df)

sns.boxplot(x='sex', y='age', data=df, hue="survived")

#Q.20 very very very hard

